{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adamdavidcole/stylegan3-fun-blend/blob/main/blend.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNa9-V2DuPXG"
      },
      "source": [
        "# StyleGAN3 Network Blending GUI\n",
        "\n",
        "A user interface for experimenting with StyleGAN3 network blending. If you're looking for StyleGAN2 blending, check out [this notebook](https://github.com/adamdavidcole/stylegan2-ada-pytorch-adam/blob/main/network_blending_gui.ipynb).\n",
        "\n",
        "Select your source and destination models and play with various blend settings and sliders."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNUlrgLAzMp9"
      },
      "source": [
        "## Setup Libraries and Google Drive Connection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ReFjlT7CvqrF",
        "outputId": "9d86210d-18ae-4276-a080-79f7c5a08776"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU 0: Tesla P100-PCIE-16GB (UUID: GPU-8174d7d1-6c95-7509-eef8-76f5df686e11)\n"
          ]
        }
      ],
      "source": [
        "# Check GPU connection\n",
        "!nvidia-smi -L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXr8jWKVvq-E",
        "outputId": "91cf3fc7-0591-47ee-8cf4-636904c2faf1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Connect Google Drive\n",
        "# WARNING: only run this if you'd like to save the results in your gdrive!\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMpqnKu7v5og"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "!pip install gdown --upgrade\n",
        "\n",
        "if os.path.isdir(\"/content/drive/MyDrive/stylegan3-fun-blend\"):\n",
        "    %cd \"/content/drive/MyDrive/stylegan3-fun-blend\"\n",
        "elif os.path.isdir(\"/content/drive/\"):\n",
        "    #install script\n",
        "    %cd \"/content/drive/MyDrive/\"\n",
        "    !git clone https://github.com/adamdavidcole/stylegan3-fun-blend.git\n",
        "    %cd stylegan3-fun-blend\n",
        "    !mkdir downloads\n",
        "    !mkdir datasets\n",
        "    !mkdir pretrained\n",
        "    # !gdown --id 1-5xZkD8ajXw1DdopTkH_rAoCsD72LhKU -O /content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/pretrained/wikiart.pkl\n",
        "else:\n",
        "    !git clone https://github.com/adamdavidcole/stylegan3-fun-blend.git\n",
        "    %cd stylegan3-fun-blend\n",
        "    !mkdir downloads\n",
        "    !mkdir datasets\n",
        "    !mkdir pretrained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2iq2jrmxUAY",
        "outputId": "6d122426-a097-4303-cc6c-1f383c2ac67e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updating 59cff72..ffc78c4\n",
            "Fast-forward\n",
            "^C\n",
            "Saved working directory and index state WIP on main: ffc78c4 end of day save\n"
          ]
        }
      ],
      "source": [
        "# ONLY IF NECESSARY: pull new code files into drive repo \n",
        "# !git config --global user.name \"test\"\n",
        "# !git config --global user.email \"test@test.com\"\n",
        "# !git fetch origin\n",
        "# !git pull\n",
        "# !git stash\n",
        "# !git checkout origin/main -- \"*.py\" \n",
        "# !git checkout origin/main -- \"*.ipynb\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dKoUIUz86ac"
      },
      "outputs": [],
      "source": [
        "!pip install einops ninja gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58xfSUxE66_W"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import torch\n",
        "import dnnlib\n",
        "from dnnlib.util import format_time\n",
        "import legacy\n",
        "import PIL.Image\n",
        "\n",
        "from torch_utils import gen_utils\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvkHcM_pxkz7"
      },
      "source": [
        "## Helper Functions & Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2fLwyWzJVQP"
      },
      "outputs": [],
      "source": [
        "#common functions \n",
        "import pickle, torch, PIL, copy, cv2, math\n",
        "import numpy as np\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "from google.colab import files\n",
        "from io import BytesIO\n",
        "from PIL import Image, ImageEnhance\n",
        "\n",
        "from IPython.display import Image as DisplayImage, clear_output\n",
        "\n",
        "# define device to use\n",
        "device = torch.device('cuda')\n",
        "\n",
        "def get_model(path):\n",
        "  # with open(path, 'rb') as f:\n",
        "  #   _G = pickle.load(f)['G_ema'].cuda()\n",
        "  device = torch.device('cuda')\n",
        "  with dnnlib.util.open_url(path) as fp:\n",
        "      _G = legacy.load_network_pkl(fp)['G_ema'].requires_grad_(False).to(device)\n",
        "  \n",
        "  return _G\n",
        "\n",
        "#tensor to PIL image \n",
        "def t2i(t):\n",
        "  return PIL.Image.fromarray((t*127.5+127).clamp(0,255)[0].permute(1,2,0).cpu().numpy().astype('uint8'))\n",
        "\n",
        "#stack an array of PIL images horizontally\n",
        "def add_imgs(images):\n",
        "  widths, heights = zip(*(i.size for i in images))\n",
        "\n",
        "  total_width = sum(widths)\n",
        "  max_height = max(heights)\n",
        "\n",
        "  new_im = PIL.Image.new('RGB', (total_width, max_height))\n",
        "\n",
        "  x_offset = 0\n",
        "  for im in images:\n",
        "    new_im.paste(im, (x_offset,0))\n",
        "    x_offset += im.size[0]\n",
        "  return new_im\n",
        "\n",
        "\n",
        "def apply_mask(matrix, mask, fill_value):\n",
        "    masked = np.ma.array(matrix, mask=mask, fill_value=fill_value)\n",
        "    return masked.filled()\n",
        " \n",
        "def apply_threshold(matrix, low_value, high_value):\n",
        "    low_mask = matrix < low_value\n",
        "    matrix = apply_mask(matrix, low_mask, low_value)\n",
        " \n",
        "    high_mask = matrix > high_value\n",
        "    matrix = apply_mask(matrix, high_mask, high_value)\n",
        " \n",
        "    return matrix\n",
        "\n",
        "# A simple color correction script to brighten overly dark images\n",
        "def simplest_cb(img, percent):\n",
        "    assert img.shape[2] == 3\n",
        "    assert percent > 0 and percent < 100\n",
        " \n",
        "    half_percent = percent / 200.0\n",
        " \n",
        "    channels = cv2.split(img)\n",
        " \n",
        "    out_channels = []\n",
        "    for channel in channels:\n",
        "        assert len(channel.shape) == 2\n",
        "        # find the low and high precentile values (based on the input percentile)\n",
        "        height, width = channel.shape\n",
        "        vec_size = width * height\n",
        "        flat = channel.reshape(vec_size)\n",
        " \n",
        "        assert len(flat.shape) == 1\n",
        " \n",
        "        flat = np.sort(flat)\n",
        " \n",
        "        n_cols = flat.shape[0]\n",
        " \n",
        "        low_val  = flat[math.floor(n_cols * half_percent)-1]\n",
        "        high_val = flat[math.ceil( n_cols * (1.0 - half_percent))-1]\n",
        " \n",
        " \n",
        "        # saturate below the low percentile and above the high percentile\n",
        "        thresholded = apply_threshold(channel, low_val, high_val)\n",
        "        # scale the channel\n",
        "        normalized = cv2.normalize(thresholded, thresholded.copy(), 0, 255, cv2.NORM_MINMAX)\n",
        "        out_channels.append(normalized)\n",
        " \n",
        "    return cv2.merge(out_channels)\n",
        " \n",
        "def normalize(inf, thresh):\n",
        "    img = np.array(inf)\n",
        "    out_img = simplest_cb(img, thresh)\n",
        "    return PIL.Image.fromarray(out_img)\n",
        "\n",
        "def get_w_from_path(w_path):\n",
        "  projected_w_np = np.load(projected_w_path)[0]\n",
        "  w = torch.tensor(projected_w_np).to(device).unsqueeze(0)\n",
        "  return w\n",
        "\n",
        "def synthesize_tensor_from_w(G, w):\n",
        "  # print(w.shape)\n",
        "  # print(w)\n",
        "  return G.synthesis(w, noise_mode='const', force_fp32=True)\n",
        "\n",
        "def synthesize_img_from_w(G, w):\n",
        "  tensor = synthesize_tensor_from_w(G, w)\n",
        "  return t2i(tensor)\n",
        "\n",
        "def synthesize_tensor_from_w_path(G, w_path):\n",
        "  w = get_w_from_path(w_path)\n",
        "  return synthesize_tensor_from_w(G, w)\n",
        "\n",
        "def synthesize_img_from_w_path(G, w_path):\n",
        "  tensor = synthesize_tensor_from_w_path(G, w_path)\n",
        "  return t2i(tensor)\n",
        "\n",
        "def synthesize_img_from_w_path(G, w_path):\n",
        "  tensor = synthesize_tensor_from_w_path(G, w_path)\n",
        "  return t2i(tensor)\n",
        "\n",
        "def synthesize_img_from_w_np(G, w_np):\n",
        "  w = torch.tensor(w_np).to(device).unsqueeze(0)\n",
        "  tensor = synthesize_img_from_w(G, w)\n",
        "  return t2i(tensor)\n",
        "\n",
        "def synthesize_img_from_seed(G, seed):\n",
        "  rnd = np.random.RandomState(seed)\n",
        "  z = torch.tensor(rnd.randn(1,G.z_dim)).cuda()\n",
        "\n",
        "  w = G.mapping(z, None, truncation_psi=0.7, truncation_cutoff=8)\n",
        "  return synthesize_img_from_w(G, w)\n",
        "\n",
        "class color:\n",
        "   PURPLE = '\\033[95m'\n",
        "   CYAN = '\\033[96m'\n",
        "   DARKCYAN = '\\033[36m'\n",
        "   BLUE = '\\033[94m'\n",
        "   GREEN = '\\033[92m'\n",
        "   YELLOW = '\\033[93m'\n",
        "   RED = '\\033[91m'\n",
        "   BOLD = '\\033[1m'\n",
        "   UNDERLINE = '\\033[4m'\n",
        "   END = '\\033[0m'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tk6FfsqFxsG2"
      },
      "source": [
        "## Model Selection\n",
        "\n",
        "**Select a source and destination model.** \n",
        "\n",
        "Keep in mind that the destination model needs to be fine-tuned from the source model for the blend to work. \n",
        "\n",
        "We will use `FFHQ_R_1024` -> `METFACES_R_1024` for this example, but feel free to paste in links to other pairs of models you'd like to expriment with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "13jk58d8JbuV"
      },
      "outputs": [],
      "source": [
        "#@title Select Source and Destination Networks  {run: \"auto\"}\n",
        "#Download pretrained checkpoint\n",
        "\n",
        "# StyleGAN 256 \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/stylegan3-r-ffhqu-256x256.pkl\"\n",
        "# StyleGAN 1024 \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/stylegan3-r-ffhqu-1024x1024.pkl\" \n",
        "\n",
        "source_model = \"FFHQ_R_1024\" #@param [\"FFHQU_R_1024\", \"FFHQ_R_1024\", \"FFHQU_T_1024\", \"FFHQ_T_1024\", \"FFHQU_R_256\", \"FFHQ_R_256\", \"FFHQU_T_256\", \"FFHQ_T_256\"] {allow-input: true}\n",
        "# destination_model = \"/content/stylegan3-r-metfaces-1024x1024.pkl\" #@param [\"BUTTERFLY_256\", \"BUTTERFLY_256_HALF_TRAINED\", \"KISS_HD\", \"pokemon_120\", \"BUTTERFLY_0012\", \"BUTTERFLY_0013\"] {allow-input: true}\n",
        "destination_model = \"METFACES_R_1024\" #@param [\"METFACES_R_1024\"] {allow-input: true}\n",
        "\n",
        "model_keys = {\n",
        "    \"FFHQU_R_1024\": \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/stylegan3-r-ffhqu-1024x1024.pkl\",\n",
        "    \"FFHQU_T_1024\": \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/stylegan3-t-ffhqu-1024x1024.pkl\",\n",
        "    \"FFHQ_R_1024\": \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/stylegan3-r-ffhq-1024x1024.pkl\",\n",
        "    \"FFHQ_T_1024\": \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/stylegan3-t-ffhq-1024x1024.pkl\",\n",
        "\n",
        "    \"FFHQU_R_256\": \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/stylegan3-r-ffhqu-256x256.pkl\",\n",
        "    \"FFHQU_T_256\": \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/stylegan3-t-ffhqu-256x256.pkl\",\n",
        "    \"FFHQ_R_256\": \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/stylegan3-r-ffhq-256x256.pkl\",\n",
        "    \"FFHQ_T_256\": \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/stylegan3-t-ffhq-256x256.pkl\",\n",
        "\n",
        "\n",
        "    # \"BUTTERFLY_256\": \"/content/drive/MyDrive/stylegan3/results/00033-stylegan3-r-butterflys_256_2-256x256-gpus1-batch32-gamma6.6/network-snapshot-000283.pkl\",\n",
        "    # \"BUTTERFLY_256_HALF_TRAINED\": \"/content/drive/MyDrive/stylegan3/results/00008-stylegan3-r-butterflys_256_2-256x256-gpus1-batch32-gamma6.6/network-snapshot-000020.pkl\",\n",
        "    # \"BUTTERFLY_0012\": \"/content/drive/MyDrive/stylegan3/results/00009-stylegan3-r-butterflys_256_2-256x256-gpus1-batch32-gamma6.6/network-snapshot-000012.pkl\",\n",
        "    # \"BUTTERFLY_0013\": \"/content/drive/MyDrive/stylegan3/results/00010-stylegan3-r-butterflys_256_2-256x256-gpus1-batch32-gamma6.6/network-snapshot-000001.pkl\",\n",
        "\n",
        "\n",
        "    'METFACES_R_1024': 'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/stylegan3-r-metfaces-1024x1024.pkl',\n",
        "\n",
        "    # \"KISS_HD\": \"/content/drive/MyDrive/stylegan3/results/00048-stylegan3-r-kiss_hd_square-gpus1-batch8-gamma6.6/network-snapshot-001216.pkl\",\n",
        "\n",
        "    # \"pokemon_120\": \"/content/drive/MyDrive/stylegan3-fun-blend/results/pokemon/training/00003-stylegan3-r-pokemon_256-gpus1-batch32-gamma6.6-resume_custom/network-snapshot-000120.pkl\"\n",
        "}\n",
        "\n",
        "# source_model_pkl = \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/stylegan3-r-ffhqu-256x256.pkl\" #@param  {type:\"string\"}\n",
        "# destination_model_pkl = \"/content/drive/MyDrive/stylegan3/results/00033-stylegan3-r-butterflys_256_2-256x256-gpus1-batch32-gamma6.6/network-snapshot-000283.pkl\" #@param {type: \"string\"}\n",
        "\n",
        "source_model_pkl = model_keys[source_model] if source_model in model_keys else source_model\n",
        "destination_model_pkl = model_keys[destination_model] if destination_model in model_keys else destination_model\n",
        "\n",
        "G = get_model(source_model_pkl)\n",
        "G_new = copy.deepcopy(G)\n",
        "G_tuned = get_model(destination_model_pkl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcUDbKAOYLBd"
      },
      "source": [
        "## Optional: Image File Selection and Projection\n",
        "\n",
        "If you'd like to project an image of a real face, follow the steps below. This process is experimental and results are often unsatisfactory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "6cca4cd146e14530bb8ba067fd6d1837",
            "872787c888464d48a45df62d2c48924b",
            "a92e7484542d4b0fab9b27055f52bb7a",
            "6a9ea36d33fa41d49e8853b0ab654cf1",
            "899e3926246d4d2eb56927e503b775dd"
          ]
        },
        "id": "0_paKVnEYhTh",
        "outputId": "b8df6d5d-9a40-4acb-b930-acfeba29a370"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6cca4cd146e14530bb8ba067fd6d1837",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Button(description='Upload New File', style=ButtonStyle())"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6a9ea36d33fa41d49e8853b0ab654cf1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Output()"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#@title Select or Upload an Image to Project {run: \"auto\"}\n",
        "#@markdown (Note press the play button first to see the upload option)\n",
        "projection_source_images_outdir = \"projection_source_images\"\n",
        "projection_source_vectors_outdir = \"projection_source_vectors\"\n",
        "\n",
        "if not os.path.isdir(projection_source_images_outdir):\n",
        "  !mkdir -p $projection_source_images_outdir\n",
        "\n",
        "def upload_files():\n",
        "  filepaths = []\n",
        "  from google.colab import files\n",
        "  uploaded = files.upload()\n",
        "  for k, v in uploaded.items():\n",
        "    k = k.replace(\" \", \"_\")\n",
        "    filepath = f\"{projection_source_images_outdir}/{k}\"\n",
        "    open(filepath, 'wb').write(v)\n",
        "    filepaths.append(filepath)\n",
        "  return list(filepaths)[0]\n",
        "\n",
        "\n",
        "button = widgets.Button(description=\"Upload New File\")\n",
        "output = widgets.Output()\n",
        "\n",
        "img_file_path = \"\" #@param {type: \"string\"}\n",
        "\n",
        "def on_button_clicked(b):\n",
        "  global img_file_path\n",
        "  # Display the message within the output widget.\n",
        "  # with output:\n",
        "  img_file_path = upload_files();\n",
        "  clear_output(wait=True)\n",
        "  display(DisplayImage(img_file_path))\n",
        "\n",
        "  print(color.BOLD + color.YELLOW + \"NOTE: \" + color.END + color.END +  \"to save this upload for future runs, copy the line below into img_file_path param in the form\");\n",
        "  print(\"\\t\" + img_file_path)\n",
        "\n",
        "  print(\"\");\n",
        "  display(button, output)\n",
        "\n",
        "if img_file_path:\n",
        "  display(DisplayImage(img_file_path))\n",
        "\n",
        "button.on_click(on_button_clicked)\n",
        "display(button, output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ps_TU1_5SwA9"
      },
      "source": [
        "### OPTIONAL: Image Enhancements\n",
        "Use these sliders to improve the image quality or amplify specific colors before projection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "cRDM5XzNSCHW"
      },
      "outputs": [],
      "source": [
        "#@title Image enhancement options {run: \"auto\"}\n",
        "#@markdown (Click **\"Save Image\"** when done)\n",
        "img = Image.open(img_file_path)\n",
        "img_out = img\n",
        "\n",
        "color_factor = 1.2 #@param {type: \"slider\", min:0, max:5, step: 0.1}\n",
        "contrast_factor = 1.2 #@param {type: \"slider\", min:0, max:5, step: 0.1}\n",
        "sharpness_factor = 1.2 #@param {type: \"slider\", min:0, max:5, step: 0.1}\n",
        "brightness_factor = 1 #@param {type: \"slider\", min:0, max:5, step: 0.1}\n",
        "\n",
        "red_factor = 1 #@param {type: \"slider\", min:0, max:5, step: 0.01}\n",
        "green_factor = 1 #@param {type: \"slider\", min:0, max:5, step: 0.01}\n",
        "blue_factor = 1 #@param {type: \"slider\", min:0, max:5, step: 0.01}\n",
        "\n",
        "img_out = ImageEnhance.Color(img_out).enhance(color_factor)\n",
        "img_out = ImageEnhance.Contrast(img_out).enhance(contrast_factor)\n",
        "img_out = ImageEnhance.Sharpness(img_out).enhance(sharpness_factor)\n",
        "img_out = ImageEnhance.Brightness(img_out).enhance(brightness_factor)\n",
        "\n",
        "img_out_np = np.array(img_out).astype(np.float64)\n",
        "img_out_np[:,:,0] *= red_factor\n",
        "img_out_np[:,:,1] *= green_factor\n",
        "img_out_np[:,:,2] *= blue_factor\n",
        "\n",
        "img_out_np = np.clip(img_out_np, 0, 255)\n",
        "img_out = Image.fromarray(img_out_np.astype(np.uint8))\n",
        "\n",
        "\n",
        "display(add_imgs([img,img_out]))\n",
        "\n",
        "def on_button_clicked(b):\n",
        "  global img_file_path\n",
        "  head, tail = os.path.split(img_file_path)\n",
        "  name, ext = os.path.splitext(tail)\n",
        "  new_file_path = f\"{head}/{name}_enhanced{ext}\"\n",
        "\n",
        "  img_out.save(new_file_path)\n",
        "\n",
        "  img_file_path = new_file_path\n",
        "\n",
        "  print(color.BOLD + color.YELLOW + \"NOTE: \" + color.END + color.END +  \"to use this image in future runs, copy the line below into img_file_path param in the form above\");\n",
        "  print(\"\\t\" + new_file_path)\n",
        "\n",
        "button = widgets.Button(description=\"Save Image\")\n",
        "button.on_click(on_button_clicked)\n",
        "print(\"\")\n",
        "display(button, output)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSZtm4pKhM9b"
      },
      "source": [
        "### Projection Into GAN Space\n",
        "\n",
        "Generate a projection into the GAN latent by either:\n",
        "  1. Entering a path to an existing projection numpy file\n",
        "  2. Running the projection script on the image selected above\n",
        "\n",
        "**Note:** run *either* step 1 or step 2 (not both)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SE-jRPaYq4C9"
      },
      "outputs": [],
      "source": [
        "#@title EITHER: Select a path to an existing projection {run: \"auto\"}\n",
        "projected_w_path = \"\" #@param {type: \"string\"}\n",
        "\n",
        "# if projected_w_path:\n",
        "#   im1 = synthesize_img_from_w_path(G, projected_w_path)\n",
        "#   im2 = synthesize_img_from_w_path(G_tuned, projected_w_path)\n",
        "#   im = add_imgs([im1, im2])\n",
        "#   display(im)\n",
        "\n",
        "if projected_w_path:\n",
        "  projected_w_np = np.load(projected_w_path)\n",
        "  print(projected_w_np.shape)\n",
        "  projected_w =  torch.tensor(projected_w_np).to(device)\n",
        "  synth_tensor = G_new.synthesis(projected_w, noise_mode='const')\n",
        "  img = PIL.Image.fromarray((synth_tensor*127.5+127).clamp(0,255)[0].permute(1,2,0).cpu().numpy().astype('uint8'))\n",
        "# t2i(synth)\n",
        "# synth_image = gen_utils.w_to_img(G, dlatents=projected_w, noise_mode='const')[0]\n",
        "# img = PIL.Image.fromarray(synth, 'RGB')\n",
        "\n",
        "  display(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "UXh5vyRThL1b"
      },
      "outputs": [],
      "source": [
        "#@title OR: Create a new projection from selected photo\n",
        "#@markdown (This process can take over 10 minuets )\n",
        "projection_source_vectors_outdir = \"projection_source_vectors\"\n",
        "\n",
        "num_steps =  1000#@param {type: \"integer\", min:0, max:10000, step:1 }\n",
        "trunc = 0.2 #@param {type: \"slider\", min:0, max:1, step:0.01 }\n",
        "loss_paper = \"sgan2\" #@param ['sgan2', 'im2sgan', 'discriminator', 'clip']\n",
        "\n",
        "# --save-video off --project-in-wplus\n",
        "!python projector.py --outdir=$projection_source_vectors_outdir --target=$img_file_path --trunc=$trunc --loss-paper=$loss_paper --num-steps=$num_steps --project-in-wplus --stabilize-projection \\\n",
        "    --cfg=stylegan3-r --network=$source_model_pkl\n",
        "print(\"Projection complete! \\n\\n\")\n",
        "\n",
        "last_projected_source_vector_dir= os.listdir(projection_source_vectors_outdir)[-1]\n",
        "last_projected_source_vector_path = f\"{projection_source_vectors_outdir}/{last_projected_source_vector_dir}\"\n",
        "\n",
        "projected_w_filename = [f for f in os.listdir(last_projected_source_vector_path) if f.endswith('.npy')][0]\n",
        "projected_w_path = f\"{last_projected_source_vector_path}/{projected_w_filename}\"\n",
        "\n",
        "print(f\"Projected w at {projected_w_path}\")\n",
        "\n",
        "np\n",
        "\n",
        "im1 = synthesize_img_from_w_path(G, projected_w_path)\n",
        "im2 = synthesize_img_from_w_path(G_tuned, projected_w_path)\n",
        "im = add_imgs([im1, im2])\n",
        "\n",
        "im.save(f\"{last_projected_source_vector_path}/proj_w_tuned.jpg\")\n",
        "\n",
        "display(im)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4CX73iny7my"
      },
      "source": [
        "### EXPERIMENTAL: Laten Vector Tuning\n",
        "Experiment with shifting along random latent directions. This ended up not having much use for the blending application but is interesting to play with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kqobTj5y-5C"
      },
      "outputs": [],
      "source": [
        "#@title fine tune projected W {run: \"auto\"}\n",
        "fine_tune_random_seed = 660 #@param {type: \"slider\", min:0, max: 1000, step: 1}\n",
        "fine_tune_magnitude = -0.00632  #@param {type: \"slider\", min:-5, max: 5, step: 0.00001 }\n",
        "w = get_w_from_path(projected_w_path)\n",
        "\n",
        "torch.manual_seed(fine_tune_random_seed)\n",
        "print(torch.rand(w.shape, device=device))\n",
        "w_fine_tuned = w + ((torch.rand(w.shape, device=device)-0.5)*2) * fine_tune_magnitude\n",
        "\n",
        "# print(w_fine_tuned)\n",
        "\n",
        "img0 = synthesize_img_from_w(G, w)\n",
        "img1 = synthesize_img_from_w(G, w_fine_tuned)\n",
        "img2 = synthesize_img_from_w(G_tuned, w_fine_tuned)\n",
        "display(add_imgs([img0, img1, img2]))\n",
        "# np_arr = w.cpu().detach().numpy()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBRNexl7Yes_"
      },
      "source": [
        "## Interactive Blending\n",
        "Below are three different UIs for blending between the source and destination networks increasing in complexity and control. \n",
        "\n",
        "I recommend starting with the EZ Blend options to get a sense of the results and then moving down to Fine Tuned Blending for more controlled outputs. The final Overblending UI is experimental.\n",
        "\n",
        "Select `use_projected_w` in the forms below *only* if you've run through the projections steps above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5b0uj_G0Pb3"
      },
      "source": [
        "### Setup Blend Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIfvsMMfLix7"
      },
      "outputs": [],
      "source": [
        "#Blend using mask. Number of layers in stylegan3 depends on config and not on gen resolution, as it was with stylegan2.\n",
        "\n",
        "# blend = [0,0,0,0,0,0,0,0.2,0.5,0.7,0.9,1,1,1,1] # 15\n",
        "# blend = [0,0,0,0,0,0,0,0.2,0.5,0.7,0.8,.8,.8,.8,.8] # 15\n",
        "# blend = [0,0,0,0,0,0.2,0.2,0.2,0.5,0.7,0.8,.8,.8,.8,1]\n",
        "# blend = [0]*7+[0.8]*(15-7)\n",
        "\n",
        "# Not blending affine layers gives us colors closer to the original gen, without affecting the geometry much.\n",
        "def doBlend(blend_mask, blend_affine_layers): \n",
        "  newDictSynt = G_tuned.synthesis.state_dict().copy()\n",
        "  # newDictSynt = G.synthesis.state_dict().copy()\n",
        "  GSyntKeys = G.synthesis.state_dict().keys()\n",
        "\n",
        "  for key in GSyntKeys:\n",
        "    # print(key)\n",
        "\n",
        "    if key[:1]!='L': continue\n",
        "    \n",
        "    l = blend_mask[int(key.split('_')[0][1:])]\n",
        "    \n",
        "    if not blend_affine_layers:\n",
        "      if 'affine' in key: l = 0\n",
        "    \n",
        "    # print(key)\n",
        "    newDictSynt[key] = G_tuned.synthesis.state_dict()[key]*l + G.synthesis.state_dict()[key]*(1-l)\n",
        "    # newDictSynt[key] = G.synthesis.state_dict()[key]\n",
        "\n",
        "\n",
        "  G_new.synthesis.load_state_dict(newDictSynt)\n",
        "  # print(G_new)\n",
        "\n",
        "# doBlend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAH3OgU6Fcp-"
      },
      "source": [
        "### EZ Blending \n",
        "\n",
        "Simple blend functions between the source and desination models. Select a seed value and blend mode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "1fA745mdGSa-"
      },
      "outputs": [],
      "source": [
        "#@title  { run: \"auto\" }\n",
        "#@markdown **Select source vector**\n",
        "use_projected_w = False #@param {type:\"boolean\"}\n",
        "#@markdown If not using a projected w, select a seed\n",
        "seed =  2782 #@param {type:\"slider\", min:0, max:10000, step:1}\n",
        "only_display_blended =  False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown --- \n",
        "#@markdown **Play with network belnding sliders** <br/>\n",
        "#@markdown (first sliders control lower level features, last sliders contol higher level features)\n",
        "\n",
        "\n",
        "# blend_thresh = 0 #@param {type:\"slider\", min:0, max:13, step:1}\n",
        "\n",
        "# Standard Blend (0-1)\n",
        "blend_affine_layers = True #@param {type:\"boolean\"}\n",
        "psi = 0.5 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "\n",
        "# blend_val_0 = 0.2 #@param {type:\"slider\", min:0, max:1, step:0.001}\n",
        "# blend_val_1 = 0.19 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "# blend_val_2 = 0.2 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "# blend_val_3 = 0.2 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "# blend_val_4 = 0.47 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "# blend_val_5 = 0.47 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "# blend_val_6 = 0.47 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "# blend_val_7 = 0.47 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "# blend_val_8 = 0.48 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "# blend_val_9 = 0.48 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "# blend_val_10 = 0.48 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "# blend_val_11 = 0.8 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "# blend_val_12 = 0 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "# blend_val_13 = 0 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "# blend_val_14 = 0 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "\n",
        "blend_mode = \"coarse_medium_fine\" #@param [\"20%\", \"50%\", \"70%\", \"halfway_cross_over\", \"halfway_cross_over_inverse\", \"coarse_medium_fine\", \"coarse_medium_fine_inverse\", \"ramp_up\", \"ramp_down\"]\n",
        "\n",
        "blend_modes = {\n",
        "    \"20%\": [0.2, 0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2, 0.2, 0.2],\n",
        "    \"50%\": [0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,],\n",
        "    \"70%\": [0.7,0.7,0.7,0.7,0.7,0.7,0.7,0.7,0.7,0.7,0.7,0.7,0.7,0.7,0.7],\n",
        "\n",
        "    \"halfway_cross_over\": [0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0],\n",
        "    \"halfway_cross_over_inverse\": [1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],\n",
        "\n",
        "\n",
        "    \"coarse_medium_fine\": [0,0,0,0,0,0.5,0.5,0.5,0.5,0.5,1,1,1,1,1],\n",
        "    \"coarse_medium_fine_inverse\": [1,1,1,1,1,0.5,0.5,0.5,0.5,0.5,0,0,0,0,0],\n",
        "\n",
        "\n",
        "    \"ramp_up\": [0,0,0,0,0,0.2,0.2,0.2,0.5,0.7,0.8,.8,.8,.8,1],\n",
        "    \"ramp_down\": [1,1,1,1,1,0.8,0.8,0.8,0.5,0.3,0.2,.2,.2,.2,0]\n",
        "}\n",
        "\n",
        "# blend = [0,0,0,0,0,0.2,0.2,0.2,0.5,0.7,0.8,.8,.8,.8,1]\n",
        "blend = blend_modes[blend_mode]\n",
        "\n",
        "if use_projected_w:\n",
        "  w = get_w_from_path(projected_w_path)\n",
        "  print(f\"W: {projected_w_path}\")\n",
        "\n",
        "else:\n",
        "  bl_str = ('_').join([str(o) for o in blend])\n",
        "  net = destination_model_pkl.split('/')[-1]\n",
        "  rnd = np.random.RandomState(seed)\n",
        "  z = torch.tensor(rnd.randn(1,G.z_dim)).cuda()\n",
        "\n",
        "  w = G.mapping(z, None, truncation_psi=psi, truncation_cutoff=8)\n",
        "\n",
        "  print(f\"Seed: {seed}, Psi: {psi}\")\n",
        "\n",
        "\n",
        "doBlend(blend, blend_affine_layers)\n",
        "\n",
        "print(blend)\n",
        "\n",
        "if only_display_blended:\n",
        "  im = G_new.synthesis(w, noise_mode='const', force_fp32=True)\n",
        "  im = t2i(im)\n",
        "  display(im)\n",
        "else:\n",
        "  im1 = G.synthesis(w, noise_mode='const', force_fp32=True)\n",
        "  im2 = G_tuned.synthesis(w, noise_mode='const', force_fp32=True)\n",
        "  im3 = G_new.synthesis(w, noise_mode='const', force_fp32=True)\n",
        "\n",
        "  im1 = t2i(im1)\n",
        "  im2 = t2i(im2)\n",
        "  im3 = t2i(im3)\n",
        "  im = add_imgs([im1, im3, im2])\n",
        "  display(im)\n",
        "# im.save(f'/content/m{net}_psi{psi}_b{bl_str}_s{seed}.jpg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SArwD4bWc0mu"
      },
      "source": [
        "### Fine Tune Blending\n",
        "\n",
        "Fine tuned blending allows you to control the individual blend levels between the source and destination models. Higher sliders will control coarser, structual behavior while lower sliders control finer details like color and texture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wN4yRfMNc3do"
      },
      "outputs": [],
      "source": [
        "#@title { run: \"auto\" }\n",
        "#@markdown **Select source vector**\n",
        "use_projected_w = False #@param {type:\"boolean\"}\n",
        "#@markdown If not using a projected w, select a seed\n",
        "seed =  9539 #@param {type:\"slider\", min:0, max:10000, step:1}\n",
        "only_display_blended =  False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown --- \n",
        "#@markdown **Play with network belnding sliders** <br/>\n",
        "#@markdown (first sliders control lower level features, last sliders contol higher level features)\n",
        "\n",
        "\n",
        "# blend_thresh = 0 #@param {type:\"slider\", min:0, max:13, step:1}\n",
        "\n",
        "# Standard Blend (0-1)\n",
        "blend_affine_layers = True #@param {type:\"boolean\"}\n",
        "blend_val_0 = 0 #@param {type:\"slider\", min:0, max:1, step:0.001}\n",
        "blend_val_1 = 0 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "blend_val_2 = 0 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "blend_val_3 = 0 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "blend_val_4 = 0 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "blend_val_5 = 0.46 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "blend_val_6 = 0.46 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "blend_val_7 = 0.46 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "blend_val_8 = 0.46 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "blend_val_9 = 0.46 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "blend_val_10 = 1 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "blend_val_11 = 1 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "blend_val_12 = 1 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "blend_val_13 = 1 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "blend_val_14 = 1 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "\n",
        "\n",
        "\n",
        "# blend = [0,0,0,0,0,0.2,0.2,0.2,0.5,0.7,0.8,.8,.8,.8,1]\n",
        "blend = [blend_val_0, blend_val_1, blend_val_2, blend_val_3, blend_val_4, blend_val_5, blend_val_6, blend_val_7, blend_val_8, blend_val_9, blend_val_10, blend_val_11, blend_val_12, blend_val_13, blend_val_14]\n",
        "psi = 0.75 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "\n",
        "if use_projected_w:\n",
        "  w = get_w_from_path(projected_w_path)\n",
        "else:\n",
        "  bl_str = ('_').join([str(o) for o in blend])\n",
        "  net = destination_model_pkl.split('/')[-1]\n",
        "  rnd = np.random.RandomState(seed)\n",
        "  z = torch.tensor(rnd.randn(1,G.z_dim)).cuda()\n",
        "\n",
        "  w = G.mapping(z, None, truncation_psi=psi, truncation_cutoff=8)\n",
        "\n",
        "print(blend)\n",
        "doBlend(blend, blend_affine_layers)\n",
        "\n",
        "if only_display_blended:\n",
        "  im = G_new.synthesis(w, noise_mode='const', force_fp32=True)\n",
        "  im = t2i(im)\n",
        "  display(im)\n",
        "else:\n",
        "  im1 = G.synthesis(w, noise_mode='const', force_fp32=True)\n",
        "  im2 = G_tuned.synthesis(w, noise_mode='const', force_fp32=True)\n",
        "  im3 = G_new.synthesis(w, noise_mode='const', force_fp32=True)\n",
        "\n",
        "  im1 = t2i(im1)\n",
        "  im2 = t2i(im2)\n",
        "  im3 = t2i(im3)\n",
        "  im = add_imgs([im1, im3, im2])\n",
        "  display(im)\n",
        "# im.save(f'/content/m{net}_psi{psi}_b{bl_str}_s{seed}.jpg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdQ9jBioc_Er"
      },
      "source": [
        "### Experimental Over Blend\n",
        "Overblending is a technique where you go beyond the sensible values of 0 and 1 when blending between the source and destination. The results often collapse when the sliders go far beyond the [0-1] scale, but it's fun to play with!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2eJgLZ4xLnD_"
      },
      "outputs": [],
      "source": [
        "#@title { run: \"auto\" }\n",
        "#@markdown **Select source vector**\n",
        "use_projected_w = False #@param {type:\"boolean\"}\n",
        "#@markdown If not using a projected w, select a seed\n",
        "seed =  753 #@param {type:\"slider\", min:0, max:10000, step:1}\n",
        "\n",
        "#@markdown --- \n",
        "#@markdown **Play with network belnding sliders** <br/>\n",
        "#@markdown (first sliders control lower level features, last sliders contol higher level features)\n",
        "\n",
        "\n",
        "# blend_thresh = 0 #@param {type:\"slider\", min:0, max:13, step:1}\n",
        "\n",
        "# Standard Blend (-10-10)\n",
        "# blend_val_0 = 1 #@param {type:\"slider\", min:0, max:1, step:0.001}\n",
        "# blend_val_1 = 0.45 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "# blend_val_2 = 0.45 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "# blend_val_3 = 0.45 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "# blend_val_4 = 0.48 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "# blend_val_5 = 0.46 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "# blend_val_6 = 1 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "# blend_val_7 = 0.5 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "# blend_val_8 = 0.5 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "# blend_val_9 = 0.76 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "# blend_val_10 = 0.89 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "# blend_val_11 = 1 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "# blend_val_12 = 1 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "# blend_val_13 = 1 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "# blend_val_14 = 1 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "\n",
        "# Experimental Blend (0-1)\n",
        "blend_affine_layers = True #@param {type:\"boolean\"}\n",
        "blend_val_0 = -0.321 #@param {type:\"slider\", min:-10, max:10, step:0.001}\n",
        "blend_val_1 = -1.08 #@param {type:\"slider\", min:-10, max:10, step:0.01}\n",
        "blend_val_2 = -1.1 #@param {type:\"slider\", min:-10, max:10, step:0.01}\n",
        "blend_val_3 = -0.74 #@param {type:\"slider\", min:-10, max:10, step:0.01}\n",
        "blend_val_4 = -0.26 #@param {type:\"slider\", min:-10, max:10, step:0.01}\n",
        "blend_val_5 = -0.51 #@param {type:\"slider\", min:-10, max:10, step:0.01}\n",
        "blend_val_6 = -0.76 #@param {type:\"slider\", min:-10, max:10, step:0.01}\n",
        "blend_val_7 = -0.76 #@param {type:\"slider\", min:-10, max:10, step:0.01}\n",
        "blend_val_8 = 0 #@param {type:\"slider\", min:-10, max:10, step:0.01}\n",
        "blend_val_9 = 0.94 #@param {type:\"slider\", min:-10, max:10, step:0.01}\n",
        "blend_val_10 = 1.26 #@param {type:\"slider\", min:-10, max:10, step:0.01}\n",
        "blend_val_11 = 1.74 #@param {type:\"slider\", min:-10, max:10, step:0.01}\n",
        "blend_val_12 = 1.59 #@param {type:\"slider\", min:-10, max:10, step:0.01}\n",
        "blend_val_13 = 1.16 #@param {type:\"slider\", min:-10, max:10, step:0.01}\n",
        "blend_val_14 = 0.62 #@param {type:\"slider\", min:-10, max:10, step:0.01}\n",
        "\n",
        "\n",
        "# blend = [0,0,0,0,0,0.2,0.2,0.2,0.5,0.7,0.8,.8,.8,.8,1]\n",
        "blend = [blend_val_0, blend_val_1, blend_val_2, blend_val_3, blend_val_4, blend_val_5, blend_val_6, blend_val_7, blend_val_8, blend_val_9, blend_val_10, blend_val_11, blend_val_12, blend_val_13, blend_val_14]\n",
        "blend_standard = np.clip(blend, 0.0,1.0)\n",
        "\n",
        "psi = 0.5 #@param {type:\"slider\", min:-10, max:10, step:0.01}\n",
        "\n",
        "if use_projected_w:\n",
        "  w = get_w_from_path(projected_w_path)\n",
        "\n",
        "  print(f\"W: {'/'.join(projected_w_path.split('/')[-2:])}\")\n",
        "else:\n",
        "  bl_str = ('_').join([str(o) for o in blend])\n",
        "  net = destination_model_pkl.split('/')[-1]\n",
        "  rnd = np.random.RandomState(seed)\n",
        "  z = torch.tensor(rnd.randn(1,G.z_dim)).cuda()\n",
        "\n",
        "  w = G.mapping(z, None, truncation_psi=psi, truncation_cutoff=8)\n",
        "  print(f\"Seed: {seed}\")\n",
        "\n",
        "print(blend)\n",
        "print(blend_standard)\n",
        "\n",
        "doBlend(blend, blend_affine_layers)\n",
        "\n",
        "im1 = G.synthesis(w, noise_mode='const', force_fp32=True)\n",
        "im2 = G_tuned.synthesis(w, noise_mode='const', force_fp32=True)\n",
        "im3 = G_new.synthesis(w, noise_mode='const', force_fp32=True)\n",
        "\n",
        "doBlend(blend_standard, blend_affine_layers)\n",
        "im4 = G_new.synthesis(w, noise_mode='const', force_fp32=True)\n",
        "\n",
        "im1 = t2i(im1)\n",
        "im2 = t2i(im2)\n",
        "im3 = t2i(im3)\n",
        "im4 = t2i(im4)\n",
        "im = add_imgs([im1, im3, im4, im2])\n",
        "# im.save(f'/content/m{net}_psi{psi}_b{bl_str}_s{seed}.jpg')\n",
        "im"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSRlmJMS9b9o"
      },
      "source": [
        "## Experimental: Pix2Pix Prep\n",
        "Use the blended network you've fine tuned above to create image pairs for pix2pix training.\n",
        "\n",
        "This concept is a work in progress and has yet to be proven as effective."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOYcmfpc9beK",
        "outputId": "b3015f9e-e487-428a-9a5b-55efaacf8816"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 40000/40000 [56:01<00:00, 11.90it/s]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "key = \"face2butterfly_local\"\n",
        "gen_type = \"test\"\n",
        "pix2pix_dir = f\"pix2pix_prep/{key}/{gen_type}\"\n",
        "\n",
        "# net 16 0-10000\n",
        "# net 12 10000 - 12000\n",
        "\n",
        "\n",
        "seed_start = 20000\n",
        "img_count =  40000\n",
        "\n",
        "if not os.path.isdir(pix2pix_dir):\n",
        "  !mkdir -p $pix2pix_dir\n",
        "\n",
        "\n",
        "for i in tqdm(range(img_count)):\n",
        "\n",
        "  seed = seed_start + i\n",
        "\n",
        "  # rnd = np.random.RandomState(seed)\n",
        "  # z = torch.tensor(rnd.randn(1,G.z_dim)).cuda()\n",
        "\n",
        "  # w = G.mapping(z, None, truncation_psi=psi, truncation_cutoff=8)\n",
        "  # img = G_new.synthesis(w, noise_mode='const', force_fp32=True)\n",
        "  imgA = synthesize_img_from_seed(G, seed)\n",
        "  imgB = synthesize_img_from_seed(G_tuned, seed)\n",
        "  img_pair = add_imgs([imgB,imgA])\n",
        "\n",
        "  img_pair.save(f\"{pix2pix_dir}/{seed}.jpg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfO_gA7h3gp7"
      },
      "source": [
        "## Refrences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S55eiNFq7C6q"
      },
      "source": [
        "This notebook was created by [Adam Cole](https://www.instagram.com/adamcole.studio/) with a specific focus on building a user interface around network blending for artists to experiment with.\n",
        "\n",
        "### Sources\n",
        "- This code lives in a fork of [StyleGAN3-fun](https://github.com/PDillis/stylegan3-fun) by [@PDillis](https://github.com/Sxela) and we take advantage of their projection script and utilities\n",
        "- Setup code was based on [@dvschultz](https://github.com/dvschultz) [StyleGAN notebooks](https://github.com/dvschultz/stylegan2-ada-pytorch) \n",
        "- The idea to use a \"blend mask\" and many helper functions were taken fully from [@Sxela](https://github.com/PDillis) [stylegan3_blending](https://github.com/Sxela/stylegan3_blending) repo\n",
        "- Much of this work was inspired by Justin Pinkney's blogpost on [network blending](https://www.justinpinkney.com/stylegan-network-blending/)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "zNUlrgLAzMp9",
        "GvkHcM_pxkz7",
        "UcUDbKAOYLBd",
        "ps_TU1_5SwA9",
        "y4CX73iny7my",
        "b5b0uj_G0Pb3",
        "MdQ9jBioc_Er",
        "nSRlmJMS9b9o"
      ],
      "include_colab_link": true,
      "machine_shape": "hm",
      "name": "Copy of blend.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "vision",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "07d9f5e508bec3c3752adee78722f681196014f9755899b43f3e274f86dd0f82"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6a9ea36d33fa41d49e8853b0ab654cf1": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_899e3926246d4d2eb56927e503b775dd",
            "msg_id": "",
            "outputs": []
          }
        },
        "6cca4cd146e14530bb8ba067fd6d1837": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Upload New File",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_872787c888464d48a45df62d2c48924b",
            "style": "IPY_MODEL_a92e7484542d4b0fab9b27055f52bb7a",
            "tooltip": ""
          }
        },
        "872787c888464d48a45df62d2c48924b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "899e3926246d4d2eb56927e503b775dd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a92e7484542d4b0fab9b27055f52bb7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
